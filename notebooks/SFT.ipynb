{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from dotenv import load_dotenv #Load HuggingFace Token\n",
    "load_dotenv()\n",
    "# --- Hyperâ€‘parameters ---\n",
    "wdir = '.'\n",
    "MAX_LEN = 1024                         # truncate long GSM8K chains of thought\n",
    "OUTPUT_DIR = f'{wdir}/models/sft'              # where to write LoRA adapter & tokenizer\n",
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUM = 8                         # effective batch 32\n",
    "LR = 2e-4\n",
    "EPOCHS = 5\n",
    "# Define total training steps\n",
    "dataset_size = 10000\n",
    "effective_batch_size = BATCH_SIZE * GRAD_ACCUM  # per_device_batch_size * num_gpus * grad_accum\n",
    "TOTAL_STEP = (dataset_size // effective_batch_size + 1) * EPOCHS  # 684 steps\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Training Hyperparameters\n",
    "eval_steps = GRAD_ACCUM * 20\n",
    "early_stopping_patience = 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.models import get_model, get_tokenizer, ground_truth_reward_model\n",
    "from utils.data_loader import get_data\n",
    "from utils.utils import get_log_probs_sft, describe_rewards\n",
    "from utils.evaluation import evaluate_sft, evaluate_rewards\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# CONFIG\n",
    "batch_size = 100\n",
    "\n",
    "prompt_length = 20\n",
    "max_length = 196\n",
    "\n",
    "# Get Model\n",
    "model_name = 'google/gemma-3-270m'\n",
    "base_model = get_model(model_name).to(device)\n",
    "tok = get_tokenizer(model_name)\n",
    "tok.padding_side = 'right'\n",
    "# Get data\n",
    "base_data = get_data('train', 18000, 25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in base_model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "# 4. LoRA Config (CHANGED for Gemma)\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    # CHANGED: Target modules for Gemma\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    # CHANGED: Task type for Causal LM\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "base_model = get_peft_model(base_model, lora_config)\n",
    "base_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The optimizer will only see the trainable PEFT parameters\n",
    "optimizer = torch.optim.AdamW(\n",
    "    base_model.parameters(),\n",
    "    lr=LR,\n",
    "    betas=(0.9, 0.95),\n",
    "    eps=1e-8,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "\n",
    "# Cosine scheduler\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps = int(0.03 * TOTAL_STEP),\n",
    "    num_training_steps = TOTAL_STEP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(base_data[:dataset_size], batch_size=BATCH_SIZE, shuffle=True, collate_fn=lambda batch: batch)\n",
    "eval_dataloader = DataLoader(base_data[dataset_size:], batch_size=BATCH_SIZE, collate_fn=lambda batch: batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Early Stopping and Model Saving Variables ---\n",
    "best_eval_loss = float('inf')\n",
    "patience_counter = 0\n",
    "global_step = 0\n",
    "best_eval_loss = evaluate_sft(base_model, eval_dataloader, tok, device, prompt_length, max_length)\n",
    "print(f\"\\nStep {global_step}: Validation Loss = {best_eval_loss:.4f}\")\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    # Note: optimizer.zero_grad() is now inside the accumulation block\n",
    "\n",
    "    # Use enumerate to get the batch index 'i'\n",
    "    for i, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\")):\n",
    "        base_model.train()\n",
    "        # --- Forward Pass ---\n",
    "        batch_nll, batch_valid_tokens = get_log_probs_sft(base_model, tok, batch, device, prompt_length, max_length)\n",
    "\n",
    "        # Compute per-token average loss for the batch\n",
    "        loss = batch_nll.sum() / batch_valid_tokens.sum()\n",
    "\n",
    "        loss = loss.mean()\n",
    "\n",
    "        # --- Scale the Loss and Backpropagate ---\n",
    "        loss = loss / GRAD_ACCUM\n",
    "        loss.backward()\n",
    "\n",
    "        # --- Optimizer Step ---\n",
    "        if (global_step + 1) % GRAD_ACCUM == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(base_model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # --- Evaluation and Early Stopping Logic ---\n",
    "        # This block should be inside the optimizer step block\n",
    "        global_step += 1 # Increment global_step only when weights are updated\n",
    "        if global_step % eval_steps == 0:\n",
    "            print(f\"\\nCurrent Step {global_step}\")\n",
    "            eval_loss = evaluate_sft(base_model, eval_dataloader, tok, device, prompt_length, max_length)\n",
    "            print(f\"\\nStep {global_step}: Validation Loss = {eval_loss:.4f}\")\n",
    "\n",
    "            # 2. Save a checkpoint at every evaluation step\n",
    "            checkpoint_dir = os.path.join(OUTPUT_DIR, f\"checkpoint-{global_step}\")\n",
    "            print(f\"\\nSaving checkpoint to {checkpoint_dir}...\")\n",
    "            base_model.save_pretrained(checkpoint_dir)\n",
    "\n",
    "            if eval_loss < best_eval_loss:\n",
    "                print(f\"\\nValidation loss improved from {best_eval_loss} to {eval_loss}. Saving model...\")\n",
    "                best_model_dir = os.path.join(OUTPUT_DIR, \"best_model\")\n",
    "                base_model.save_pretrained(best_model_dir)\n",
    "                best_eval_loss = eval_loss\n",
    "                patience_counter = 0  # Reset patience\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                print(f\"\\nValidation loss did not improve. Patience: {patience_counter}/{early_stopping_patience}\")\n",
    "\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(\"\\nEarly stopping triggered.\")\n",
    "                break\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    if patience_counter >= early_stopping_patience:\n",
    "        break\n",
    "\n",
    "print(\"\\n--- Training Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = evaluate_rewards(base_model, ground_truth_reward_model, tok, eval_dataloader, prompt_length, max_length)\n",
    "describe_rewards(rewards)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
