{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.models import get_model, get_tokenizer, ground_truth_reward_model\n",
    "from utils.evaluation import evaluate_dpo, evaluate_ground_truth_rewards\n",
    "from utils.data_preprocessing import CustomDataset, transform_df_for_dpo, precompute_reference_log_probs_batched\n",
    "from utils.loss_functions import DPOLoss\n",
    "from utils.utils import get_log_probs, describe_rewards\n",
    "from utils.preference_generation import determine_preference\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from peft import LoraConfig, PeftModel\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc, os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hyperâ€‘parameters ---\n",
    "wdir = '.'\n",
    "MAX_LEN = 1024                         # truncate long GSM8K chains of thought\n",
    "QUALITY = 'low'\n",
    "OUTPUT_DIR = f\"{wdir}/models/DPO_{QUALITY}\"              # where to write LoRA adapter & tokenizer\n",
    "BATCH_SIZE = 16\n",
    "GRAD_ACCUM = 2                         # effective batch 32\n",
    "LR = 1e-5\n",
    "EPOCHS = 2\n",
    "# Define total training steps\n",
    "dataset_size = 10000\n",
    "effective_batch_size = BATCH_SIZE * GRAD_ACCUM  # per_device_batch_size * num_gpus * grad_accum\n",
    "TOTAL_STEP = (dataset_size // effective_batch_size + 1) * EPOCHS  # 684 steps\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "prompt_length = 20\n",
    "max_length = 196\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "scale = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get Model\n",
    "model_name = 'google/gemma-3-270m'\n",
    "\n",
    "policy_model = get_model(model_name).to(device)\n",
    "policy_model = PeftModel.from_pretrained(policy_model,\n",
    "                                         f'{wdir}/models/sft/best_model',\n",
    "                                         adapter_name = 'sft')\n",
    "ref_model = get_model(model_name).to(device)\n",
    "ref_model = PeftModel.from_pretrained(ref_model,\n",
    "                                     f'{wdir}/models/sft/best_model')\n",
    "\n",
    "tok = get_tokenizer(model_name)\n",
    "\n",
    "# LoRA Config (CHANGED for Gemma)\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    # CHANGED: Target modules for Gemma\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    # CHANGED: Task type for Causal LM\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "policy_model.add_adapter(\"dpo\", lora_config)  # Adds new empty trainable adapter\n",
    "# Set active adapters for composition (SFT + DPO for policy)\n",
    "policy_model.base_model.set_adapter([\"sft\", \"dpo\"])\n",
    "\n",
    "for name, param in policy_model.named_parameters():\n",
    "    if 'dpo' not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "policy_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder containing the CSV files\n",
    "path = f'{wdir}/data/'\n",
    "\n",
    "# Get a list of all CSV files in the folder\n",
    "all_files = glob.glob(path + f\"dpo_data_{QUALITY}.csv\")\n",
    "\n",
    "print(all_files)\n",
    "# Expected output: ['data/sales_jan.csv', 'data/sales_feb.csv', 'data/sales_mar.csv']\n",
    "# Read each CSV file into a DataFrame and store them in a list\n",
    "list_of_dfs = [pd.read_csv(file) for file in all_files]\n",
    "\n",
    "# Concatenate all DataFrames in the list by row\n",
    "df = pd.concat(list_of_dfs, ignore_index=True)\n",
    "\n",
    "df['pref'] = df.apply(determine_preference, \n",
    "                      axis=1,\n",
    "                      args=(scale, rng)  # scale=2.0, rng=custom generator)\n",
    ")\n",
    "print(df.pref.value_counts())\n",
    "\n",
    "# # --- DATA PROCESS ---\n",
    "df = transform_df_for_dpo(df)\n",
    "train_ref_y1, train_ref_y2 = precompute_reference_log_probs_batched(ref_model, tok, df[:dataset_size], effective_batch_size // 8, device, prompt_length, max_length)\n",
    "eval_ref_y1, eval_ref_y2 = precompute_reference_log_probs_batched(ref_model, tok, df[dataset_size:], effective_batch_size // 8, device, prompt_length, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create DataLoaders ---\n",
    "train_dataset = CustomDataset(df[:dataset_size], train_ref_y1, train_ref_y2)\n",
    "eval_dataset = CustomDataset(df[dataset_size:], eval_ref_y1, eval_ref_y2)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=lambda batch: batch)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=BATCH_SIZE, collate_fn=lambda batch: batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Components ---\n",
    "loss_fn = DPOLoss(0.1)\n",
    "# The optimizer will only see the trainable PEFT parameters\n",
    "optimizer = torch.optim.AdamW(\n",
    "    policy_model.parameters(),\n",
    "    lr=LR,\n",
    "    betas=(0.9, 0.95),\n",
    "    eps=1e-8,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "\n",
    "# Cosine scheduler\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps = int(0.03 * TOTAL_STEP),\n",
    "    num_training_steps = TOTAL_STEP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Early Stopping and Model Saving Variables ---\n",
    "global_step = 0\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    # Note: optimizer.zero_grad() is now inside the accumulation block\n",
    "\n",
    "    # Use enumerate to get the batch index 'i'\n",
    "    for i, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\")):\n",
    "        policy_model.train()\n",
    "        # --- Forward Pass ---\n",
    "        log_probs_y1_policy = get_log_probs(policy_model, tok, batch, 'first_responses', device, prompt_length, max_length)\n",
    "        log_probs_y2_policy = get_log_probs(policy_model, tok, batch, 'second_responses', device, prompt_length, max_length)\n",
    "\n",
    "        # Stack the tensor items\n",
    "        choices = torch.stack([item['choices'] for item in batch])\n",
    "        ref_log_probs_y1 = torch.stack([item['ref_log_probs_y1'] for item in batch])\n",
    "        ref_log_probs_y2 = torch.stack([item['ref_log_probs_y2'] for item in batch])\n",
    "\n",
    "        loss = loss_fn(\n",
    "            log_probs_y1_policy,\n",
    "            log_probs_y2_policy,\n",
    "            ref_log_probs_y1.to(device),\n",
    "            ref_log_probs_y2.to(device),\n",
    "            choices.to(device)\n",
    "        )\n",
    "\n",
    "        # --- Scale the Loss and Backpropagate ---\n",
    "        loss = loss / GRAD_ACCUM\n",
    "        loss.backward()\n",
    "\n",
    "        # --- Optimizer Step ---\n",
    "        if (global_step + 1) % GRAD_ACCUM == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(policy_model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        global_step += 1 # Increment global_step only when weights are updated\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n--- Training Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model.eval()\n",
    "rewards = evaluate_ground_truth_rewards(policy_model, ground_truth_reward_model, tok, eval_dataloader, prompt_length, max_length)\n",
    "describe_rewards(rewards)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
