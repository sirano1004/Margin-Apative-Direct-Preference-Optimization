{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11660,"status":"ok","timestamp":1759056198272,"user":{"displayName":"Gyu Rho","userId":"12982909790841439689"},"user_tz":-600},"id":"ucZjbJQRNyoS","outputId":"44b0cdd7-36af-4e98-e609-e1698d373d6b"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Device set to use cuda:0\n"]}],"source":["from utils.models import get_model, get_tokenizer, ground_truth_reward_model\n","from utils.evaluation import evaluate_ground_truth_rewards\n","from utils.data_preprocessing import CustomDataset, transform_df_for_dpo, precompute_reference_log_probs_batched\n","from utils.loss_functions import IPOLoss\n","from utils.utils import get_log_probs, describe_rewards\n","from utils.preference_generation import determine_preference\n","\n","import torch\n","from torch.utils.data import DataLoader\n","from transformers import get_cosine_schedule_with_warmup\n","from peft import LoraConfig, PeftModel\n","\n","from tqdm import tqdm\n","import pandas as pd\n","import numpy as np\n","import gc, os\n","import glob"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qz0Fzj4MNyoT"},"outputs":[],"source":["# --- Hyper‑parameters ---\n","wdir = '.'\n","MAX_LEN = 1024                         # truncate long GSM8K chains of thought\n","QUALITY = 'high'\n","OUTPUT_DIR = f\"{wdir}/models/IPO_{QUALITY}\"              # where to write LoRA adapter & tokenizer\n","BATCH_SIZE = 16\n","GRAD_ACCUM = 2                         # effective batch 32\n","LR = 1e-5\n","EPOCHS = 2\n","# Define total training steps\n","dataset_size = 10000\n","effective_batch_size = BATCH_SIZE * GRAD_ACCUM  # per_device_batch_size * num_gpus * grad_accum\n","TOTAL_STEP = (dataset_size // effective_batch_size + 1) * EPOCHS  # 684 steps\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","\n","prompt_length = 20\n","max_length = 196\n","\n","rng = np.random.default_rng(42)\n","scale = 1.0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5125,"status":"ok","timestamp":1759056203408,"user":{"displayName":"Gyu Rho","userId":"12982909790841439689"},"user_tz":-600},"id":"BaWpNkg9NyoT","outputId":"f39d3e2b-c74b-431a-8f38-c4265649d7db"},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable params: 1,474,560 || all params: 271,047,296 || trainable%: 0.5440\n"]}],"source":["\n","# Get Model\n","model_name = 'google/gemma-3-270m'\n","\n","policy_model = get_model(model_name).to(device)\n","policy_model = PeftModel.from_pretrained(policy_model,\n","                                         f'{wdir}/models/sft/best_model',\n","                                         adapter_name = 'sft')\n","ref_model = get_model(model_name).to(device)\n","ref_model = PeftModel.from_pretrained(ref_model,\n","                                     f'{wdir}/models/sft/best_model')\n","\n","tok = get_tokenizer(model_name)\n","\n","# LoRA Config (CHANGED for Gemma)\n","lora_config = LoraConfig(\n","    r=16,\n","    lora_alpha=32,\n","    # CHANGED: Target modules for Gemma\n","    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    # CHANGED: Task type for Causal LM\n","    task_type=\"CAUSAL_LM\"\n",")\n","\n","policy_model.add_adapter(\"dpo\", lora_config)  # Adds new empty trainable adapter\n","# Set active adapters for composition (SFT + DPO for policy)\n","policy_model.base_model.set_adapter([\"sft\", \"dpo\"])\n","\n","for name, param in policy_model.named_parameters():\n","    if 'dpo' not in name:\n","        param.requires_grad = False\n","\n","policy_model.print_trainable_parameters()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":199737,"status":"ok","timestamp":1759056403148,"user":{"displayName":"Gyu Rho","userId":"12982909790841439689"},"user_tz":-600},"id":"cLzaMfDtNyoU","outputId":"8eb2a8e8-6dfa-488a-bfb5-33fa2a1729aa"},"outputs":[{"name":"stdout","output_type":"stream","text":["['/content/drive/Othercomputers/My MacBook Pro/Oracle DPO/data/dpo_data_high.csv']\n","pref\n","1 > 2    6021\n","2 > 1    5979\n","Name: count, dtype: int64\n","Pre-computing reference log probabilities in batches...\n"]},{"name":"stderr","output_type":"stream","text":["Batches: 100%|██████████| 313/313 [02:44<00:00,  1.90it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Pre-computing reference log probabilities in batches...\n"]},{"name":"stderr","output_type":"stream","text":["Batches: 100%|██████████| 63/63 [00:32<00:00,  1.92it/s]\n"]}],"source":["# Path to the folder containing the CSV files\n","path = f'{wdir}/data/'\n","\n","# Get a list of all CSV files in the folder\n","all_files = glob.glob(path + f\"dpo_data_{QUALITY}.csv\")\n","\n","print(all_files)\n","# Expected output: ['data/sales_jan.csv', 'data/sales_feb.csv', 'data/sales_mar.csv']\n","# Read each CSV file into a DataFrame and store them in a list\n","list_of_dfs = [pd.read_csv(file) for file in all_files]\n","\n","# Concatenate all DataFrames in the list by row\n","df = pd.concat(list_of_dfs, ignore_index=True)\n","\n","df['pref'] = df.apply(determine_preference,\n","                      axis=1,\n","                      args=(scale, rng)  # scale=2.0, rng=custom generator)\n",")\n","print(df.pref.value_counts())\n","\n","# # --- DATA PROCESS ---\n","df = transform_df_for_dpo(df)\n","train_ref_y1, train_ref_y2 = precompute_reference_log_probs_batched(ref_model, tok, df[:dataset_size], effective_batch_size, device, prompt_length, max_length)\n","eval_ref_y1, eval_ref_y2 = precompute_reference_log_probs_batched(ref_model, tok, df[dataset_size:], effective_batch_size, device, prompt_length, max_length)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xY1pFc63NyoU"},"outputs":[],"source":["# --- Create DataLoaders ---\n","train_dataset = CustomDataset(df[:dataset_size], train_ref_y1, train_ref_y2)\n","eval_dataset = CustomDataset(df[dataset_size:], eval_ref_y1, eval_ref_y2)\n","train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=lambda batch: batch)\n","eval_dataloader = DataLoader(eval_dataset, batch_size=BATCH_SIZE, collate_fn=lambda batch: batch)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xeZ0jIkENyoV"},"outputs":[],"source":["# --- Training Components ---\n","loss_fn = IPOLoss(0.1)\n","# The optimizer will only see the trainable PEFT parameters\n","optimizer = torch.optim.AdamW(\n","    policy_model.parameters(),\n","    lr=LR,\n","    betas=(0.9, 0.95),\n","    eps=1e-8,\n","    weight_decay=0.01\n",")\n","\n","\n","# Cosine scheduler\n","scheduler = get_cosine_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps = int(0.03 * TOTAL_STEP),\n","    num_training_steps = TOTAL_STEP\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1326517,"status":"ok","timestamp":1759057729675,"user":{"displayName":"Gyu Rho","userId":"12982909790841439689"},"user_tz":-600},"id":"Y4FAl36uNyoV","outputId":"087a1306-bb0d-4f90-807d-b3103f8caee8"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","--- Starting Training ---\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1: 100%|██████████| 625/625 [11:01<00:00,  1.06s/it]\n","Epoch 2: 100%|██████████| 625/625 [11:04<00:00,  1.06s/it]"]},{"name":"stdout","output_type":"stream","text":["\n","--- Training Finished ---\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# --- Early Stopping and Model Saving Variables ---\n","global_step = 0\n","print(\"\\n--- Starting Training ---\")\n","for epoch in range(EPOCHS):\n","\n","    # Note: optimizer.zero_grad() is now inside the accumulation block\n","\n","    # Use enumerate to get the batch index 'i'\n","    for i, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\")):\n","        policy_model.train()\n","        # --- Forward Pass ---\n","        log_probs_y1_policy = get_log_probs(policy_model, tok, batch, 'first_responses', device, prompt_length, max_length)\n","        log_probs_y2_policy = get_log_probs(policy_model, tok, batch, 'second_responses', device, prompt_length, max_length)\n","\n","        # Stack the tensor items\n","        choices = torch.stack([item['choices'] for item in batch])\n","        ref_log_probs_y1 = torch.stack([item['ref_log_probs_y1'] for item in batch])\n","        ref_log_probs_y2 = torch.stack([item['ref_log_probs_y2'] for item in batch])\n","\n","        loss = loss_fn(\n","            log_probs_y1_policy,\n","            log_probs_y2_policy,\n","            ref_log_probs_y1.to(device),\n","            ref_log_probs_y2.to(device),\n","            choices.to(device)\n","        )\n","\n","        # --- Scale the Loss and Backpropagate ---\n","        loss = loss / GRAD_ACCUM\n","        loss.backward()\n","\n","        # --- Optimizer Step ---\n","        if (global_step + 1) % GRAD_ACCUM == 0:\n","            torch.nn.utils.clip_grad_norm_(policy_model.parameters(), 1.0)\n","            optimizer.step()\n","            scheduler.step()\n","            optimizer.zero_grad()\n","\n","        global_step += 1 # Increment global_step only when weights are updated\n","\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","\n","print(\"\\n--- Training Finished ---\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":326621,"status":"ok","timestamp":1759059105602,"user":{"displayName":"Gyu Rho","userId":"12982909790841439689"},"user_tz":-600},"id":"PGZ6yklKNyoV","outputId":"cded7f50-04d1-417f-e7ce-7269abaaee77"},"outputs":[{"name":"stderr","output_type":"stream","text":["You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"]},{"name":"stdout","output_type":"stream","text":["Mean: 0.3542\n","Standard Deviation: 2.1090\n","95% Confidence Interval: (0.2618, 0.4467)\n"]}],"source":["policy_model.eval()\n","rewards = evaluate_ground_truth_rewards(policy_model, ground_truth_reward_model, tok, eval_dataloader, prompt_length, max_length)\n","describe_rewards(rewards)"]},{"cell_type":"markdown","metadata":{"id":"q-hDd8IVuMnf"},"source":["# Reults\n","### Low qualty\n","You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","Mean: 0.1041\n","Standard Deviation: 2.1338\n","95% Confidence Interval: (0.0106, 0.1976)\n","\n","## medium quality\n","You seem to be using. thepipelines sequentially. onGPU. In order to maximize efficiency please use a dataset\n","Mean: 0.3115\n","Standard Deviation: 2.1212\n","95% Confidence Interval: (0.2185, 0.4045)\n","\n","## high quality\n","You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","Mean: 0.3542\n","Standard Deviation: 2.1090\n","95% Confidence Interval: (0.2618, 0.4467)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
